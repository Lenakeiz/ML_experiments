{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import sklearn as skl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "# Importing the dataset\r\n",
    "dataset = pd.read_csv('Data.csv')\r\n",
    "X = dataset.iloc[:, :-1].values\r\n",
    "y = dataset.iloc[:, 3].values\r\n",
    "\r\n",
    "print(y)\r\n",
    "print(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n",
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 nan]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' nan 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Removing Nans"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "print(skl.__version__) # The model for the tutorial \r\n",
    "\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "imp = SimpleImputer(missing_values = np.nan, strategy='mean')\r\n",
    "imputer = imp.fit(X[:, 1:3])\r\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\r\n",
    "\r\n",
    "print(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.0.1\n",
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transforming categorical"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "\r\n",
    "# Transforming the categorical data\r\n",
    "from sklearn.compose import ColumnTransformer\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(),[0])], remainder='passthrough') #index of the column, passthrough keeps the other columns\r\n",
    "\r\n",
    "X = np.array(ct.fit_transform(X))\r\n",
    "\r\n",
    "from sklearn.preprocessing import LabelEncoder\r\n",
    "le = LabelEncoder()\r\n",
    "y = le.fit_transform(y)\r\n",
    "\r\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Splitting dataset into Training and Test set\r\n",
    "## Usually you apply the feature scaling **after** the split. Splitting the dataset consist in making two existing set. The feature scaling applied from before does not make sense before you will get information leaking from the training set. In this way your model will not be right for performing. Reccomended for splitting is 80/20"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "# Splitting\r\n",
    "\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) # fixing the seed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature scaling\r\n",
    "## It is essential to avoid some of the features would weight more than others.\r\n",
    "## Feture scaling can be achieved by doing standardisation or normalisation. Standardisation will results in -3 +3 range. Normalsation it is always a positive number and between 0 and 1. Normalisation is recommended in normal distribution. Standardisation works every time becuse calculates the standard deviation of the data.\r\n",
    "\r\n",
    "## The feature scaling will be applied to the x train and the transformation will be performed **also** to X_test. That means the learner will only be able to learn the transform on the training data.\r\n",
    "\r\n",
    "## You don t have to apply the transformation to the dummy variables (there are already in a good range)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "from sklearn.preprocessing import StandardScaler\r\n",
    "ss = StandardScaler()\r\n",
    "X_train[:, 3:] = ss.fit_transform(X_train[:, 3:]) # 3: starts from 3 and take all the remaining columns\r\n",
    "X_test[:, 3:] = ss.transform(X_test[:, 3:]) # applied the transform applied to our training test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(X_train)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "print(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "e16b573a426c71c0ebe04e4f28ac2539e5d98b1d565f8bf1acf240a5e7b140c7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}